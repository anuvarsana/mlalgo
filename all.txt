import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df=pd.read_csv(r"D:\fifth sem\ml labtest1\labtest-2\play-tennis-with_continuous_features.csv")
df.drop(['Day'],inplace=True,axis=1)
df
features=df.columns[:-1]
features
df.info()
df.describe()
df['PlayTennis'].value_counts()
discrete=df.select_dtypes(include=['object']).columns[:-1]
discrete
continuous=df.select_dtypes(include=['int64','float64']).columns
continuous
X=df.iloc[:,:-1]
Y=df.iloc[:,-1]

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)
train_data=np.concatenate((np.array(x_train),np.array(y_train).reshape(-1,1)),axis=1)
test_data=np.concatenate((np.array(x_test),np.array(y_test).reshape(-1,1)),axis=1)


train_data=pd.DataFrame(train_data,columns=list(features)+['PlayTennis'])
test_data=pd.DataFrame(test_data,columns=list(features)+['PlayTennis'])
train_data
test_data
#Prior probabilities
y_poss=df['PlayTennis'].unique()
y_poss
prior_proba={}
for i in y_poss:
    prior_proba[i]=float(len(train_data[train_data['PlayTennis']==i]))/len(train_data)
    
prior_proba
CPM for discrete
cpm_discrete={}
for i in discrete:
    cpm_discrete[i]={}
    for j in train_data[i].unique():
        cpm_discrete[i][j]={}
        for k in y_poss:
            cpm_discrete[i][j][k]=float(len(train_data[(train_data[i]==j)&(train_data['PlayTennis']==k)]))/len(train_data[train_data['PlayTennis']==k])
            
cpm_discrete
alpha=0.1
for i in discrete:
    for j in train_data[i].unique():
        for k in y_poss:
            if(cpm_discrete[i][j][k]==0):
                for s in train_data[i].unique():

                    cpm_discrete[i][s][k]=float(len(train_data[(train_data['PlayTennis']==k)&(train_data[i]==s)])+alpha)/(len(train_data[train_data['PlayTennis']==k])+alpha*len(train_data[i].unique()))
cpm_discrete
continuous cpm
continuous_cpm={}
for i in continuous:
    continuous_cpm[i]={}
    continuous_cpm[i]['mean']={}
    continuous_cpm[i]['std']={}
    for j in y_poss:
        continuous_cpm[i]['mean'][j]=train_data[train_data['PlayTennis']==j][i].mean()
        continuous_cpm[i]['std'][j]=train_data[train_data['PlayTennis']==j][i].std()
        
continuous_cpm
y_pred=[]
for i in range(len(test_data)):
    pred={}
    denom=0
    for j in y_poss:
        sum=prior_proba[j]
        for k in discrete:
            sum*=cpm_discrete[k][test_data.iloc[i][k]][j]
        for k in continuous:
            proba=1.0/(continuous_cpm[k]['std'][j]*(2*np.pi)**0.5)*np.exp(-(test_data.iloc[i][k]-continuous_cpm[k]['mean'][j])**2/(2*continuous_cpm[k]['std'][j]**2))
            sum*=proba
        denom+=sum
    for j in y_poss:
        numerator=prior_proba[j]
        for k in discrete:
            numerator*=cpm_discrete[k][train_data.iloc[i][k]][j]
        for k in continuous:
            proba=1.0/(continuous_cpm[k]['mean'][j]*(2*np.pi)**0.5)*np.exp(-(train_data.iloc[i][k]-continuous_cpm[k]['mean'][j])**2/(2*continuous_cpm[k]['std'][j]**2))
            numerator*=proba
        pred[j]=float(numerator)/denom
    y_pred.append(max(pred,key=pred.get))
y_pred
print(np.concatenate((np.array(y_pred).reshape(-1,1),np.array(y_test).reshape(-1,1)),axis=1))
from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,recall_score,precision_score

c=confusion_matrix(y_pred=y_pred,y_true=y_test)
c
print(accuracy_score(y_pred,y_test))
print(recall_score(y_pred,y_test,average='weighted'))
print(f1_score(y_pred=y_pred,y_true=y_test,average='weighted'))





dis_breast
import pandas as pd
import numpy as np
df=pd.read_csv('breast-cancer-wisconsin.csv')
print("Null values: ",(df =='?').sum().sum())
print((df =='?').sum())
for i in df:
    m=df[i].mode()[0]
    df[i]=df[i].replace('?',m)
x=df.iloc[:,1:-1]
y=df.iloc[:,-1]
df.head()
classes=df['class'].unique()
print(classes)
from sklearn.model_selection import train_test_split
train,test=train_test_split(df,test_size=0.2,random_state=0)

cpt={}
for i in train.columns[1:-1]:
    cpt[i]={}
    for j in train[i].unique():
      cpt[i][j]={}
      for k in classes:
          den= train[train.iloc[:, -1] == k].shape[0]
          num = train[(train[i] == j) & (train.iloc[:, -1] == k)].shape[0]
          cpt[i][j][k] = num/den
print(cpt)
prior = {}
for i in classes:
  prior[i] = train.iloc[:,-1].value_counts()[i] / len(train)
print(prior)

alpha=1
for i in train.columns[1:-1]:
    zf=[]
    for j in train[i].unique():
      for k in classes:
          if (cpt[i][j][k]==0):
              zf.append(k)
    for k in zf:
      for j in train[i].unique():
        den= train[train.iloc[:, -1] == k].shape[0]+alpha*len(train[i].unique())
        num = train[(train[i] == j) & (train.iloc[:, -1] == k)].shape[0]+alpha
        cpt[i][j][k] = num/den

print(cpt)
print(test)
y_predict=[]
y_test=test.iloc[:,-1].tolist()
for i in range(len(test)):
  posterior={}
  for j in classes:
    posterior[j]=prior[j]
    for k in test.columns[1:-1]:
      posterior[j]*=cpt[k][test.iloc[i][k]][j]
  total_prob = sum(posterior.values())
  for k in classes:
      posterior[k] /= total_prob
  y_predict.append(max(posterior, key=posterior.get))

cm=[[0,0],[0,0]]
for i in range(len(y_predict)):
  if y_predict[i]==2:
    if y_test[i]==2:
      cm[0][0]+=1
    else:
      cm[0][1]+=1
  else:
    if y_test[i]==2:
      cm[1][0]+=1
    else:
      cm[1][1]+=1
print(cm)
print("Accuracy: ",(cm[0][0]+cm[1][1])/len(y_predict))




con_guass
import pandas as pd
import numpy as np

col=["sepal_length","sepal_width","petal_length","petal_width","class"]
df=pd.read_csv("iris.data",names=col)
df.head()
classes=df["class"].unique()
classes
from sklearn.model_selection import train_test_split
train,test=train_test_split(df,test_size=0.2,random_state=0)


cpt={}
for i in train.columns[:-1]:
    cpt[i]={}
    for k in classes:
        cpt[i][k]={}
        a=train[train['class'] == k]
        if len(a)!=0:
          cpt[i][k]["mean"]=a[i].mean()
          cpt[i][k]["std"]=a[i].std()
        else:
          cpt[i][k]["mean"]=0
          cpt[i][k]["std"]=0

print(cpt)
prior = {}
for i in classes:
  prior[i] = train.iloc[:,-1].value_counts()[i] / len(train)
print(prior)
y_predict=[]

for i in range(len(test)):
  posterior={}
  for k in classes:
    posterior[k]=prior[k]
    for j in test.columns[:-1]:
      den=(np.sqrt(2*np.pi))*cpt[j][k]["std"]
      num=np.exp(-(test.iloc[i][j]-cpt[j][k]["mean"]) ** 2 / (2 * cpt[j][k]["std"] ** 2))
      posterior[k]*=num/den
  total=sum(posterior.values())
  for k in posterior:
    posterior[k]/=total
  y_predict.append(max(posterior,key=posterior.get))

print(len(y_predict))
print(len(test.iloc[:,-1]))

predictions = pd.Series(y_predict)
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
# Confusion Matrix
cm = confusion_matrix(test.iloc[:,-1],predictions)
print("Confusion Matrix:\n", cm)

accuracy = accuracy_score(test.iloc[:,-1], predictions)
print("Accuracy:", accuracy)




knn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv("C:/Users/91822/Desktop/college/ML lab/iris.csv")
from sklearn.model_selection import train_test_split

X=df.iloc[:,:-1].values
Y=df.iloc[:,-1].values

x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)


distances=[]
for i in range(x_test.shape[0]):
    distances.append([])
    for j in range(x_train.shape[0]):
        dis=0
        for k in range(x_train.shape[1]):
            dis+=(x_train[j][k]-x_test[i][k])**2
        dis=dis**0.5
        distances[i].append((dis,y_train[j]))

distances
def sort_tuple(row):
    row.sort(key=lambda x:x[0])
for i in range(x_test.shape[0]):
    sort_tuple(distances[i])
distances[0]
k=5
nearest_neighbours=[]
for i in range(x_test.shape[0]):
    nearest_neighbours.append([])
    for j in range(k):
        nearest_neighbours[i].append(distances[i][j])
nearest_neighbours
y_pred=[]
for i in range(x_test.shape[0]):
    votes={}
    for j in df['class'].unique():
        votes[j]=0
    for j in range(k):
        class_p=nearest_neighbours[i][j][1]
        votes[class_p]+=1
    y_pred.append(max(votes,key=votes.get))

print(np.concatenate((np.array(y_pred).reshape(-1,1),np.array(y_test).reshape(-1,1)),axis=1))
from sklearn.metrics import confusion_matrix,accuracy_score

print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))




knnall

import numpy as np
import sympy as sp
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
df =  pd.read_csv('Iris_dataset.csv', index_col=0)
df
features = df.columns[:-1].to_list()
target = df.columns[-1]

print('Features:', features)
print('Target:', target)
classes = df.loc[:, target].unique()

print('Classes:', classes)
df.describe()
df.groupby('Class').describe().transpose()
print("Correlation matrix: ")
print(df.iloc[:, :-1].corr())
sns.heatmap(df.iloc[:, :-1].corr())
plt.show()
sns.boxplot(df.iloc[:, :-1])
plt.show()
for i in df.groupby('Class'):
    sns.boxplot(i[1].iloc[:, :-1])
    plt.title(i[0])
    plt.show()

# Outliers (Class-wise)
for i in classes:
    for j in features:
        t = df.query(f'Class == \'{i}\'').loc[:, j]
        Q1 = t.quantile(0.25)
        Q3 = t.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        '''
        lower_bound = t.mean() - 3 * t.std()
        upper_bound = t.mean() + 3 * t.std()
        '''

        index = (t < lower_bound) | (t > upper_bound)
        outliers = t[index]

        if len(outliers) != 0:
            print(f'{j} outliers in {i} class')
            print(outliers.to_list())
            df.drop(outliers.index, axis=0, inplace=True)

print('Outliers are removed')
print('Number of data points in the dataset: ', len(df))
sns.pairplot(df, hue='Class')
plt.show()
fig = px.scatter_3d(df, x='Sepal_Length', y='Sepal_Width', z='Petal_Width', color='Class')
fig.show()
fig = px.scatter_3d(df, x='Sepal_Length', y='Sepal_Width', z='Petal_Width', color='Petal_Length', symbol='Class')
fig.show()
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df.loc[:, features], df.loc[:, target], test_size=0.1, random_state=37)
# KNN with k = 5
k = 5

import math
from statistics import mode

def first(x):
    return x[0]

y_pred = []

for i in X_test.index:
    dist_df = []
    for j in X_train.index:
        d = math.dist(X_test.loc[i, features], X_train.loc[j, features])
        dist_df.append([d, y_train.loc[j]])

    dist_df.sort(key=first)
    y_pred.append(mode([i[1] for i in dist_df[:k]]))
y_pred
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

ConfusionMatrixDisplay(confusion_matrix(y_pred=y_pred, y_true=y_test), display_labels=classes).plot()
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_true=y_test, y_pred=y_pred))
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(X=X_train, y=y_train)

y_pred = knn.predict(X=X_test)

print(classification_report(y_true=y_test, y_pred=y_pred))
